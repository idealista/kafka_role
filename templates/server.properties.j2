##
# {{ ansible_managed }}

############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
{% if kafka_hosts is defined %}
{% for server in kafka_hosts %}
{% if server.host is defined %}
{% if server.host == inventory_hostname %}
broker.id={{ server.id }}
{% endif %}
{% else %}
broker.id=0
{% endif %}
{% endfor %}
{% else %}
broker.id=0
{% endif %}

{% if kafka_delete_topic_enable is defined %}
delete.topic.enable={{ kafka_delete_topic_enable }}
{% endif %}

{% if kafka_auto_create_topics is defined %}
auto.create.topics.enable={{ kafka_auto_create_topics }}
{% endif %}

############################# Socket Server Settings #############################

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = security_protocol://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
{% if kafka_listeners is defined %}
listeners={% set listener_str = '' %}
{%- for listener in kafka_listeners %}
{%- set listener_str = listener_str ~ listener.name ~ '://' ~ listener.host %}{{ listener_str }}{{ "," if not loop.last else "" }}
{%- endfor %}
{% else %}
listeners=PLAINTEXT://{{ kafka_host_name }}:{{ kafka_port }}
{% endif %}


{% if kafka_listeners is defined %}
# Map between listener names and security protocols. This must be defined for the same security protocol to be usable in more than one port or IP. For example, internal and external traffic can be separated even if SSL is required for both. Concretely, the user could define listeners with names INTERNAL and EXTERNAL and this property as: `INTERNAL:SSL,EXTERNAL:SSL`. As shown, key and value are separated by a colon and map entries are separated by commas. Each listener name should only appear once in the map. Different security (SSL and SASL) settings can be configured for each listener by adding a normalised prefix (the listener name is lowercased) to the config name. For example, to set a different keystore for the INTERNAL listener, a config with name listener.name.internal.ssl.keystore.location would be set. If the config for the listener name is not set, the config will fallback to the generic config (i.e. ssl.keystore.location). Note that in KRaft a default mapping from the listener names defined by controller.listener.names to PLAINTEXT is assumed if no explicit mapping is provided and no other security protocol is in use.
listener.security.protocol.map={% set listener_str = '' %}
{%- for listener in kafka_listeners %}
{%- set listener_str = listener_str ~ listener.name ~ ':' ~ listener.protocol %}{{ listener_str }}{{ "," if not loop.last else "" }}
{%- endfor %}
{% endif %}

{% if kafka_inter_broker_listener_name is defined %}
# Name of listener used for communication between brokers. If this is unset, the listener name is defined by security.inter.broker.protocol. It is an error to set this and security.inter.broker.protocol properties at the same time.
inter.broker.listener.name={{ kafka_inter_broker_listener_name }}
{% endif %}

{% if kafka_control_plane_listener_name is defined %}
# Name of listener used for communication between brokers. If this is unset, the listener name is defined by security.inter.broker.protocol. It is an error to set this and security.inter.broker.protocol properties at the same time.
control.plane.listener.name={{ kafka_control_plane_listener_name }}
{% endif %}

{% if kafka_listeners is defined %}
# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
advertised.listeners={% set listener_str = '' %}
{%- for listener in kafka_listeners %}
{%- if listener.advertised_host is defined %}
{%- set listener_str = listener_str ~ listener.name ~ '://' ~ listener.advertised_host %}{{ listener_str }}{{ "," if not loop.last else "" }}
{%- endif %}
{%- endfor %}
{% endif %}

# The number of threads handling network requests #
num.network.threads={{ kafka_num_network_threads }}

# The number of threads doing disk I/O
num.io.threads={{ kafka_num_io_threads }}

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes={{ kafka_socket_send_buffer_bytes }}

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes={{ kafka_socket_receive_buffer_bytes }}

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes={{ kafka_socket_request_max_bytes }}

############################# Security  #############################

# The list of SASL mechanisms enabled in the Kafka server. The list may contain any mechanism for which a security provider is available. Only GSSAPI is enabled by default.
{% if kafka_sasl_enabled_mechanisms is defined %}
sasl.enabled.mechanisms={{ kafka_sasl_enabled_mechanisms }}
{% endif %}

{% if kafka_sasl_mechanism_inter_broker_protocol is defined %}
sasl.mechanism.inter.broker.protocol={{ kafka_sasl_mechanism_inter_broker_protocol }}
{% endif %}

{% if kafka_allow_everyone_if_no_acl_found is defined %}
allow.everyone.if.no.acl.found={{ kafka_allow_everyone_if_no_acl_found }}
{% endif %}

{% if kafka_authorizer_class_name is defined %}
authorizer.class.name={{ kafka_authorizer_class_name }}
{% endif %}

{% if kafka_listeners_sasl_jaas_config is defined %}
{# listener.name.clients.plain.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
    username="admin" \
    password="admin-secret" \
    user_admin="admin-secret"; #}
{% for listener_config in kafka_listeners_sasl_jaas_config %}
listener.name.{{ listener_config.name|lower }}.{{ listener_config.mechanism|lower }}.sasl.jaas.config={{ listener_config.module }} required \
{% if listener_config.broker_auth is defined %}
    username="{{ listener_config.broker_auth.user }}" \
    password="{{ listener_config.broker_auth.pass }}" \
{% endif %}
{% if listener_config.acl_auth is defined %}
{% for user in listener_config.acl_auth %}
    user_{{ user.user }}="{{ user.pass }}"{{ ";" if loop.last else " \\"}}
{% endfor %}
{% endif %}
{% endfor %}
{% endif %}

{% if kafka_super_users is defined %}
super.users={{ ['User:'] | product(kafka_super_users) | map('join') | list | product([';']) | map('join') | list | join }}
{% endif %}


############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
offsets.topic.replication.factor={{ kafka_offsets_topic_replication_factor }}
transaction.state.log.replication.factor={{ kafka_transaction_state_log_replication_factor }}
transaction.state.log.min.isr={{ kafka_transaction_state_log_min_isr }}


############################# Log Basics #############################

# A comma seperated list of directories under which to store log files
log.dirs={{ kafka_data_path }}

# The number of logical partitions per topic per server. More partitions allow greater parallelism
# for consumption, but also mean more files.
num.partitions={{ kafka_num_partitions }}

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir={{ kafka_num_recovery_threads_per_data_dir }}

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
{% if kafka_log_flush_interval_messages is defined %}
log.flush.interval.messages={{ kafka_log_flush_interval_messages }}
{% endif %}

# The maximum amount of time a message can sit in a log before we force a flush
{% if kafka_log_flush_interval_ms is defined %}
log.flush.interval.ms={{ kafka_log_flush_interval_ms }}
{% endif %}

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion
log.retention.hours={{ kafka_log_retention_hours }}

# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
# segments dont drop below log.retention.bytes.
log.retention.bytes={{ kafka_log_retention_bytes }}

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes={{ kafka_log_segment_bytes }}

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms={{ kafka_log_retention_check_interval_ms }}

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect={{ kafka_zookeeper_hosts | join(',') }}

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms={{ kafka_zookeeper_connection_timeout_ms }}

{% if kafka_zookeeper_sasl_enabled is defined %}
zookeeper.sasl.enabled={{ kafka_zookeeper_sasl_enabled }}
{% endif %}

############################# Extra Properties #############################
{% if kafka_extra_properties is defined %}
{% for config in kafka_extra_properties %}
{{ config.key }}={{ config.value }}
{% endfor %}
{% endif %}
