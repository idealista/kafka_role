---
## General

kafka_version: 2.6.0
kafka_scala_version: 2.12  # Recommended

## Service options

kafka_private_tmp: true

# Owner
kafka_user: kafka
kafka_group: kafka

# start on boot
kafka_service_enabled: true
# current state: started, stopped
kafka_service_state: started
kafka_service_state_timeout: 300
kafka_service_file_path: /etc/systemd/system/kafka.service

# Files & Paths
kafka_install_path: /opt/kafka
kafka_conf_path: /etc/kafka
kafka_data_path: "{{ kafka_install_path }}/data"
kafka_log_path: /var/log/kafka

# Logging
kafka_log_level: WARN
kafka_gc_log_enabled: false

# JVM
kafka_xmx: "{{ (ansible_memtotal_mb / 2) | int }}m"
kafka_xms: "{{ (ansible_memtotal_mb / 2) | int }}m"
kafka_jmx_port: 9010
kafka_opts: ""
kafka_jvm_performance_opts: ""

# Service properties

# The id of the broker. This must be set to a unique integer for each broker.
# List of dict (i.e. {kafka_hosts:[{host:,id:},{host:,id:},...]})
kafka_hosts:
  - host: "{{ inventory_hostname }}"  # the machine running
    id: 0
# Switch to enable topic deletion or not, default value is false
kafka_delete_topic_enable: 'false'

# Switch to enable auto create topic or not, default value is true
kafka_auto_create_topics: 'true'

kafka_topics: []
kafka_not_removable_topics: ['KSTREAM-AGGREGATE-STATE-STORE']  # Contains filter

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = security_protocol://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
# listeners: PLAINTEXT://{{kafka_host}}:{{kafka_port}}
kafka_host_name: "{{ ansible_nodename }}"
kafka_port: 9092

# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
# kafka_advertised_listeners: PLAINTEXT://your.host.name:9092

# The number of threads handling network requests
kafka_num_network_threads: 3
# The number of threads doing disk I/O
kafka_num_io_threads: 8
# The send buffer (SO_SNDBUF) used by the socket server
kafka_socket_send_buffer_bytes: 102400
# The receive buffer (SO_RCVBUF) used by the socket server
kafka_socket_receive_buffer_bytes: 102400
# The maximum size of a request that the socket server will accept (protection against OOM)
kafka_socket_request_max_bytes: 104857600

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
kafka_num_partitions: 1
# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
kafka_num_recovery_threads_per_data_dir: 1

# The number of messages to accept before forcing a flush of data to disk
kafka_log_flush_interval_messages: 10000
# The maximum amount of time a message can sit in a log before we force a flush
kafka_log_flush_interval_ms: 1000

# The minimum age of a log file to be eligible for deletion
kafka_log_retention_hours: 168
# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
# segments don't drop below log.retention.bytes.
kafka_log_retention_bytes: 1073741824
# The maximum size of a log segment file. When this size is reached a new log segment will be created.
kafka_log_segment_bytes: 1073741824
# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
kafka_log_retention_check_interval_ms: 300000

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
kafka_zookeeper_hosts:
  - localhost:2181
# Timeout in ms for connecting to zookeeper
kafka_zookeeper_connection_timeout_ms: 6000

# The replication factor for the offsets topic (set higher to ensure availability).
# Internal topic creation will fail until the cluster size meets this replication factor requirement.
kafka_offsets_topic_replication_factor: 3

# The replication factor for the transaction topic (set higher to ensure availability).
# Internal topic creation will fail until the cluster size meets this replication factor requirement.
kafka_transaction_state_log_replication_factor: 3

# Overridden min.insync.replicas config for the transaction topic.
kafka_transaction_state_log_min_isr: 2

## Miscellaneous
kafka_force_reinstall: false

### Templates path
kafka_log4j_template_path: log4j.properties.j2
kafka_server_template_path: server.properties.j2
kafka_service_template_path: kafka.service.j2

## Extra properties
kafka_topics_config: []
# - name: 'test'
#   delete.retention.ms: 100000
#   max.message.bytes: 1024
kafka_extra_properties: []
# - key: message.max.bytes
#   value: 409715200

kafka_mirror: "https://archive.apache.org/dist/kafka"
kafka_package_name: "kafka_{{ kafka_scala_version }}-{{ kafka_version }}"
kafka_package: "{{ kafka_package_name }}.tgz"
kafka_sources_url: "{{ kafka_mirror }}/{{ kafka_version }}/{{ kafka_package }}"


## Agent configuration (optional)
kafka_agents_required_libs:
  - unzip
  - tar
  - apt-transport-https

kafka_agents_force_reinstall: false

# kafka_agents_config:
#   - name: "agent_name"
#     download_url: "download_url"
#     version: "x.x.x"
#     java_opts:
#       - '-javaagent:{{ kafka_root_path }}/agent_name/agent_file'
#     configuration_files:
#       - "configuration_file.yml"
#     params: {
#       application_name: "application_name",
#       license_key: "license_key"
#     }
